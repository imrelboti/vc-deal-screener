# ğŸ¤– VC Deal Screener - Phase 2: Collecte Automatique de DonnÃ©es

**SystÃ¨me complet d'automatisation avec Machine Learning pour collecter et enrichir automatiquement les donnÃ©es des startups marocaines.**

---

## ğŸ“‹ Table des MatiÃ¨res

1. [Vue d'Ensemble](#vue-densemble)
2. [Architecture](#architecture)
3. [Installation](#installation)
4. [Configuration](#configuration)
5. [Utilisation](#utilisation)
6. [Sources de DonnÃ©es](#sources-de-donnÃ©es)
7. [Machine Learning](#machine-learning)
8. [API REST](#api-rest)
9. [DÃ©ploiement](#dÃ©ploiement)
10. [Monitoring](#monitoring)
11. [CoÃ»ts](#coÃ»ts)

---

## ğŸ¯ Vue d'Ensemble

### Objectif
Collecter **automatiquement** les donnÃ©es de 500+ startups marocaines depuis multiples sources, nettoyer, enrichir avec ML, et stocker dans PostgreSQL.

### FonctionnalitÃ©s

âœ… **Collecte Multi-Sources**
- Crunchbase API (donnÃ©es officielles)
- Web Scraping intelligent (sites marocains)
- Google Search API (dÃ©couverte)
- Sources locales (incubateurs, mÃ©dias)
- LinkedIn (optionnel via Bright Data)

âœ… **Machine Learning**
- Classification sectorielle automatique
- Scoring prÃ©dictif (0-100)
- Extraction d'entitÃ©s (founders, tech stack)
- Analyse de sentiment des news

âœ… **Automatisation**
- Collecte hebdomadaire complÃ¨te
- Mise Ã  jour quotidienne
- Actualisation toutes les 6h
- DÃ©duplication intelligente

âœ… **QualitÃ© des DonnÃ©es**
- Nettoyage automatique
- Validation
- DÃ©tection de doublons (85% similaritÃ©)
- Score de qualitÃ© par startup

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FRONTEND (React)                          â”‚
â”‚  Fichier: vc-deal-scr-FIXED.jsx                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ HTTP/REST
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  API REST (FastAPI)                          â”‚
â”‚  GET /startups, /startups/{id}, /stats                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BASE DE DONNÃ‰ES (PostgreSQL)                    â”‚
â”‚  Tables: startups, metrics, news, funding_rounds            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚ Ã‰crit les donnÃ©es
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            SCHEDULER (APScheduler)                           â”‚
â”‚  - Collecte hebdomadaire: Dimanche 3h                       â”‚
â”‚  - Mise Ã  jour quotidienne: 2h                              â”‚
â”‚  - Actualisation rapide: Toutes les 6h                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DATA COLLECTION ORCHESTRATOR                         â”‚
â”‚  Coordonne tous les collecteurs + ML Pipeline               â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚        â”‚        â”‚        â”‚
       â–¼        â–¼        â–¼        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Crunchbaseâ”‚ â”‚  Google  â”‚ â”‚   Web    â”‚ â”‚  Local   â”‚
â”‚   API    â”‚ â”‚ Search   â”‚ â”‚ Scraper  â”‚ â”‚ Sources  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   ML PIPELINE       â”‚
         â”‚ - Classification    â”‚
         â”‚ - Scoring           â”‚
         â”‚ - NER               â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   DATA CLEANER      â”‚
         â”‚ - Normalisation     â”‚
         â”‚ - DÃ©duplication     â”‚
         â”‚ - Validation        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Installation

### PrÃ©requis

- Python 3.9+
- PostgreSQL 14+
- Redis (optionnel, pour cache)
- Docker & Docker Compose (recommandÃ©)

### MÃ©thode 1: Docker (RecommandÃ©)

```bash
# 1. Cloner le projet
git clone <your-repo>
cd automation

# 2. Copier et configurer .env
cp .env.example .env
# Ã‰diter .env avec vos API keys

# 3. Lancer avec Docker Compose
docker-compose up -d

# 4. VÃ©rifier les logs
docker-compose logs -f collector
```

**C'est tout !** Le systÃ¨me tourne maintenant automatiquement.

### MÃ©thode 2: Installation Manuelle

```bash
# 1. CrÃ©er environnement virtuel
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# 2. Installer dÃ©pendances
pip install -r requirements.txt

# 3. TÃ©lÃ©charger modÃ¨le NLP franÃ§ais
python -m spacy download fr_core_news_md

# 4. Configurer PostgreSQL
createdb vc_deal_screener

# 5. Configurer .env
cp .env.example .env
# Ã‰diter avec vos credentials

# 6. Tester la connexion DB
python database/db_manager.py

# 7. Lancer le collecteur
python main_orchestrator.py

# 8. Lancer le scheduler (optionnel)
python scheduler/auto_collector.py
```

---

## âš™ï¸ Configuration

### Fichier .env

**Minimum requis:**

```bash
# Database
DB_HOST=localhost
DB_NAME=vc_deal_screener
DB_USER=postgres
DB_PASSWORD=your_password

# Au moins UNE API key
CRUNCHBASE_API_KEY=your_key  # RecommandÃ©
# OU
SERPER_API_KEY=your_key  # Alternative
```

**Configuration complÃ¨te:**

Voir `.env.example` pour toutes les options.

### API Keys - OÃ¹ les obtenir?

| Service | Prix | Signup | NÃ©cessaire? |
|---------|------|--------|-------------|
| **Crunchbase** | $300/mois | [data.crunchbase.com](https://data.crunchbase.com) | â­ Fortement recommandÃ© |
| **Serper** | $50/mois | [serper.dev](https://serper.dev) | âœ… RecommandÃ© |
| **Bright Data** | $50/mois | [brightdata.com](https://brightdata.com) | Optionnel |

**Budget minimal:** $300-350/mois pour fonctionnement optimal.

**Alternative low-cost:** Utiliser uniquement web scraping (gratuit mais moins fiable).

---

## ğŸ“– Utilisation

### Collecte Manuelle (Test)

```bash
# Collecte complÃ¨te immÃ©diate
python main_orchestrator.py

# Avec un seul collecteur (debug)
python collectors/crunchbase_collector.py
python collectors/google_search_collector.py
python collectors/web_scraper.py
```

### Scheduler Automatique

```bash
# DÃ©marrer le scheduler
python scheduler/auto_collector.py

# Avec collecte immÃ©diate au dÃ©marrage
python scheduler/auto_collector.py --run-now
```

**Planning par dÃ©faut:**
- âœ… **Dimanche 3h**: Collecte complÃ¨te (toutes sources)
- âœ… **Tous les jours 2h**: Mise Ã  jour incrÃ©mentale
- âœ… **Toutes les 6h**: Actualisation rapide (news)
- âœ… **Samedi 1h**: Nettoyage base de donnÃ©es

### VÃ©rifier les DonnÃ©es

```bash
# Se connecter Ã  PostgreSQL
psql -U postgres -d vc_deal_screener

# Compter les startups
SELECT COUNT(*) FROM startups;

# Voir les derniÃ¨res ajoutÃ©es
SELECT name, sector, score, created_at 
FROM startups 
ORDER BY created_at DESC 
LIMIT 10;

# Statistiques par secteur
SELECT sector, COUNT(*), AVG(score) 
FROM startups 
GROUP BY sector;
```

---

## ğŸ“Š Sources de DonnÃ©es

### 1. Crunchbase API â­

**Meilleure source** - DonnÃ©es officielles vÃ©rifiÃ©es.

```python
# Collecte automatique
from collectors.crunchbase_collector import CrunchbaseCollector

collector = CrunchbaseCollector()
startups = await collector.collect()
```

**DonnÃ©es rÃ©cupÃ©rÃ©es:**
- Nom, description
- Funding (montant, rounds)
- Ã‰quipe (nombre employÃ©s)
- Secteur d'activitÃ©
- Localisation
- Contact (email, LinkedIn)

### 2. Web Scraper Intelligent

**Sources marocaines:**
- Annuaires de startups
- MÃ©dias tech (MÃ©dias24, L'Ã‰conomiste)
- Blogs startup

```python
from collectors.web_scraper import IntelligentWebScraper

scraper = IntelligentWebScraper()
startups = await scraper.collect()
```

### 3. Google Search (Serper)

**DÃ©couverte de nouvelles startups** via recherche ciblÃ©e.

```python
from collectors.google_search_collector import GoogleSearchCollector

collector = GoogleSearchCollector()
leads = await collector.collect()
```

### 4. Sources Locales

**DonnÃ©es vÃ©rifiÃ©es:**
- Portfolios incubateurs (DARE Inc, etc.)
- LaurÃ©ats compÃ©titions
- Base gouvernementale

```python
from collectors.local_sources_collector import LocalSourcesCollector

collector = LocalSourcesCollector()
startups = await collector.collect()
```

---

## ğŸ§  Machine Learning

### Classification Sectorielle

**Automatique** - Classifie chaque startup dans un des 16 secteurs.

```python
from ml.classification_pipeline import MLClassificationPipeline

pipeline = MLClassificationPipeline()
await pipeline.load_models()

sector = await pipeline.classify_sector(
    "Plateforme de paiement mobile",
    "PayTech"
)
# RÃ©sultat: 'fintech'
```

**Algorithme:**
- Analyse des keywords (primary/secondary)
- PondÃ©ration par importance
- Fallback: classification ML (BERT fine-tuned)

### Scoring PrÃ©dictif

**Score 0-100** basÃ© sur 8 critÃ¨res pondÃ©rÃ©s.

```python
from ml.scoring_engine import MLScoringEngine

engine = MLScoringEngine()
score = await engine.predict_score(startup_data)

# Explication du score
explanation = engine.explain_score(startup_data)
```

**CritÃ¨res de scoring:**
- Funding amount (25%)
- Traction financiÃ¨re (30%)
- Taille Ã©quipe (15%)
- Hotness secteur (15%)
- Mentions mÃ©dias (10%)
- Partenariats (10%)
- Support gouvernemental (5%)

### Extraction d'EntitÃ©s (NER)

**Extrait automatiquement:**
- Noms des fondateurs
- Technologies utilisÃ©es
- Partenaires
- Montants de financement

```python
entities = await pipeline.extract_entities(description)
# {
#   'founders': ['Ahmed Alami', 'Sara Bennani'],
#   'technologies': ['python', 'react', 'aws'],
#   'partnerships': ['Bank X'],
#   'funding_info': {'amount': '1.5M MAD', 'round': 'seed'}
# }
```

### Analyse de Sentiment

**Sur les actualitÃ©s** - Score -1 (nÃ©gatif) Ã  +1 (positif).

```python
sentiment = await pipeline.analyze_sentiment([
    "La startup connaÃ®t une croissance exceptionnelle",
    "LevÃ©e de fonds record"
])
# RÃ©sultat: 0.85 (trÃ¨s positif)
```

---

## ğŸ”Œ API REST

### Lancer l'API

```bash
# DÃ©veloppement
uvicorn api.main:app --reload --port 8000

# Production (Docker)
docker-compose up api
```

### Endpoints

**GET /startups**
```bash
curl http://localhost:8000/startups?sector=fintech&min_score=70
```

**GET /startups/{id}**
```bash
curl http://localhost:8000/startups/1
```

**GET /stats**
```bash
curl http://localhost:8000/stats
```

**POST /collect** (Trigger manuel)
```bash
curl -X POST http://localhost:8000/collect
```

### Connecter au Frontend

```javascript
// Dans votre React app
const fetchStartups = async () => {
  const response = await fetch('http://localhost:8000/startups');
  const data = await response.json();
  return data;
};
```

---

## ğŸš¢ DÃ©ploiement

### Option 1: Railway (RecommandÃ©)

```bash
# 1. CrÃ©er compte Railway
# 2. Installer CLI
npm i -g @railway/cli

# 3. Login
railway login

# 4. Initialiser projet
railway init

# 5. Ajouter PostgreSQL
railway add postgresql

# 6. DÃ©ployer
railway up

# 7. Variables d'environnement
railway variables set CRUNCHBASE_API_KEY=your_key
```

**CoÃ»t:** ~$20-30/mois

### Option 2: Heroku

```bash
# 1. CrÃ©er app
heroku create vc-screener-collector

# 2. Ajouter PostgreSQL
heroku addons:create heroku-postgresql:mini

# 3. Configurer variables
heroku config:set CRUNCHBASE_API_KEY=your_key

# 4. DÃ©ployer
git push heroku main

# 5. Scaler worker
heroku ps:scale worker=1
```

### Option 3: VPS (Digital Ocean / AWS)

```bash
# 1. CrÃ©er Droplet Ubuntu 22.04
# 2. SSH et installer Docker
apt update && apt install docker.io docker-compose

# 3. Cloner projet
git clone <repo>
cd automation

# 4. Configurer .env
nano .env

# 5. Lancer
docker-compose up -d

# 6. Nginx reverse proxy (optionnel)
```

---

## ğŸ“ˆ Monitoring

### Logs

```bash
# Docker
docker-compose logs -f collector

# Fichiers logs
tail -f logs/collector_*.log
```

### MÃ©triques PostgreSQL

```sql
-- Performances
SELECT 
    schemaname,
    tablename,
    n_live_tup as rows,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
FROM pg_stat_user_tables
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- Collectes rÃ©centes
SELECT 
    collector_name,
    status,
    startups_collected,
    completed_at
FROM collection_logs
ORDER BY completed_at DESC
LIMIT 10;
```

### Alertes (Optionnel)

```python
# IntÃ©gration Sentry pour error tracking
import sentry_sdk

sentry_sdk.init(
    dsn=os.getenv('SENTRY_DSN'),
    environment=os.getenv('ENVIRONMENT')
)
```

---

## ğŸ’° CoÃ»ts

### Setup Initial

| Item | CoÃ»t |
|------|------|
| DÃ©veloppement | DÃ©jÃ  fait âœ… |
| Serveur test | Gratuit (local) |
| **Total** | **0â‚¬** |

### CoÃ»ts Mensuels

**Configuration Optimale:**

| Service | Prix | NÃ©cessaire? |
|---------|------|-------------|
| Crunchbase Pro API | $300/mois | â­ Oui |
| Serper (Google Search) | $50/mois | âœ… RecommandÃ© |
| Bright Data (LinkedIn) | $50/mois | Optionnel |
| Railway/Heroku Hosting | $20-30/mois | âœ… Oui |
| **Total Optimal** | **$370-430/mois** | |

**Configuration Budget:**

| Service | Prix |
|---------|------|
| Web scraping only | Gratuit |
| VPS Digital Ocean | $12/mois |
| **Total Budget** | **$12/mois** |

**ROI Potentiel:**
- Vente base de donnÃ©es: $500-2000
- Abonnement Premium: $29/mois x 50 users = $1450/mois
- Lead generation: $75/lead x 20 = $1500/mois

---

## ğŸ¯ Prochaines Ã‰tapes

### Semaine 1
- [x] Configuration environnement
- [ ] Test collecteurs individuellement
- [ ] PremiÃ¨re collecte complÃ¨te
- [ ] Validation donnÃ©es

### Semaine 2
- [ ] Tuning ML models
- [ ] Optimisation performance
- [ ] Mise en place monitoring

### Semaine 3-4
- [ ] DÃ©ploiement production
- [ ] IntÃ©gration frontend-backend
- [ ] Tests utilisateurs

---

## ğŸ“ Support

**Documentation:**
- README.md (ce fichier)
- Code comments inline
- Docstrings Python

**Troubleshooting:**
- VÃ©rifier les logs
- Tester chaque collecteur sÃ©parÃ©ment
- Valider API keys dans .env

**Performance:**
- Si lent: Augmenter `MAX_CONCURRENT_REQUESTS`
- Si erreurs: RÃ©duire concurrence, augmenter `REQUEST_TIMEOUT`

---

## ğŸ“„ Licence

MIT License - Utilisez librement

---

<div align="center">

**ğŸ‡²ğŸ‡¦ Fait avec â¤ï¸ pour l'Ã©cosystÃ¨me startup marocain**

**SystÃ¨me prÃªt Ã  collecter 500+ startups automatiquement ! ğŸš€**

</div>
